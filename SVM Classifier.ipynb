{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“Support Vector Machine” (SVM) is a **supervised machine** learning algorithm which can be used for both **classification or regression** challenges. However,  it is **mostly used in classification** **plot each data item** as a point in **n-dimensional** space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we **perform classification** by **finding** the **hyper-plane** that differentiate the two classes very well as shown below. **Support Vectors** are simply the co-ordinates of individual observation which are **closest to the hyper plane**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://cdn-images-1.medium.com/max/600/0*0o8xIA4k3gXUDCFU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there can be **multiple hyper planes** which can separate the data points but we want the hyperplane which **maximizes the margin i.e maximum distance between the support vectors.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Math behind SVM\n",
    "Check out following [video's](https://youtu.be/mA5nwGoRAOo) for detailed explanation<br><br>\n",
    "\n",
    "In order to implement the SVM from scratch, we need to understand the fundamental math behind it so that we can write code which helps us find the optimum hyperplane.As we can see from above image our key **objective is to find a optimum hyperplane which maximizes the margin**. Note that the **hyperplane** can be a **line(2D features) or a plane(for >2D features)**.\n",
    "\n",
    "Let us consider the case of 2D features, i.e the hyperplane is a line. Let the line is represented by the equation:\n",
    "\n",
    "$$ w.x +b $$ \n",
    "\n",
    "where **w** is the **slope(Weight)** of the line and **b** is the **intercept(bias)** for the line. **$x$** is the **feature space**. Our objective is the maximize the distance margin between positive and negative classes. We can formulate it as follows:\n",
    "For every $x_i$:\n",
    "\n",
    "$$ w.x_i + b >= 1$$ for $x_i$ having class 1\n",
    "$$or$$\n",
    "\n",
    "$$ w.x_i + b <= -1$$ for $x_i$ having class -1\n",
    "\n",
    "1. if w.x+b=0 then we get the decision boundary i.e optimum hyper plane\n",
    "2. if w.x+b=1 then we get (+)class hyperplane or +ve support vector\n",
    "3. if w.x+b=-1 then we get (-)class hyperplane or -ve support vector\n",
    "\n",
    "So above equations helps us set the hyper plane and decision boundary. But we also **need to maximize the margin i.e the distance between the +ve and -ve support vectors**. It gets reduced to following optimization problem:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://cdn-images-1.medium.com/max/1200/1*colCMuTP_LN4zvjfrsZcQA.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as per our problem formulation, **D1** represents the **+ve** support vector and **D2** represents the **-ve** support vector. We want to maximize the distance between these two planes which reduces to maximizing $\\frac{2}{\\vert{w}\\vert}$. This can be better understood by following equation:\n",
    "$$ Max(W) = \\frac{((X+ - X-).w)}{|w|}$$ where X+.w = 1 and X-.w = -1\n",
    "\n",
    "So we can intern minimize |w| or 1/2 $|w|^2$ making it a convex optimization as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://cdn-images-1.medium.com/max/800/1*6mcff3dEDuTFT21IvlL1_Q.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $y$ represent the the true output label with $y_+$ as 1 and $y_-$ as -1. We multiply our support vector equation with $y$ and finally get the following equation as shown above:\n",
    "$$y_i (W^Tx+b) =1$$\n",
    "\n",
    "So the loss function for any $x_i$ is given as:\n",
    "![title](https://cdn-images-1.medium.com/max/1200/1*3xErahGeTFnbDiRuNXjAuA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e **f(x)** is the predicted class based on the **sign of $W^Tx+b$.** If the **predicted sign is correct**, there is **no loss** i.e 0, else the loss is given as above. Our **objective is to reduce this loss and minimize the |w| at the same time**. So, the final constrained equation looks like this:\n",
    "![title](https://cdn-images-1.medium.com/max/1200/1*GQAd28bK8LKOL2kOOFY-tg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note $\\lambda$ acts as regularization parameter. The objective of the regularization parameter is to balance the margin maximization and loss. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients for above loss function is given as :\n",
    "![title](https://cdn-images-1.medium.com/max/800/1*WUphtYLfTOAoaXQXvImBeA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there is no misclassification, i.e our model correctly predicts the class of our data point, we only have to update the gradient from the regularization parameter.\n",
    "![title](https://cdn-images-1.medium.com/max/800/1*-nKEXrWos8Iuf-DWSv_srQ.png)\n",
    "\n",
    "When there is a misclassification, i.e our model make a mistake on the prediction of the class of our data point, we include the loss along with the regularization parameter to perform gradient update.\n",
    "![title](https://cdn-images-1.medium.com/max/800/1*tnvMhAKaTUCO43diEvtTAQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to implement SVM in code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have understood the math behind SVM and have defined what we need to optimize, lets look at the code to implement it:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialization\n",
    "We initialize the `l_rate` which is $\\alpha$ and `epoch` which is the number of iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, l_rate=.00001, epoch=100000):\n",
    "        self.l_rate = l_rate\n",
    "        self.epoch = epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fitting the data\n",
    "We initialize a weight matrix equal to the number of features. **Note we are not taking a bias term for simplicity**. Now for each x,y pair, we check the gradient update condition as discussed above and update it. Note we have kept $\\lambda$ as dynamic and it changes with each epoch. So, the updates are bigger initially and smaller subsequently. We also have a `plot` function, which plots the 2D graph of x,y and decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, x, y, plot=True):\n",
    "    self.w = np.zeros(x.shape[1])\n",
    "\n",
    "    for e in range(self.epoch):\n",
    "\n",
    "        for i, val in enumerate(x):\n",
    "            val1 = np.dot(x[i], self.w)\n",
    "            if (y[i]*val1 < 1):\n",
    "                self.w = self.w + self.l_rate * ((y[i]*x[i]) - (2*(1/(e+1))*self.w))\n",
    "            else:\n",
    "                self.w = self.w + self.l_rate * (-2*(1/(e+1))*self.w)\n",
    "\n",
    "    if plot:\n",
    "        self.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Plotting the data and decision boundary\n",
    "The following function makes a 2D plot of x,y and also the decision boundary to help us visualize the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(self, x, y):\n",
    "    for val, inp in enumerate(x):\n",
    "        if y[val] == -1:\n",
    "            plt.scatter(inp[0], inp[1], s=100, marker='_', linewidths=5)\n",
    "        else:\n",
    "            plt.scatter(inp[0], inp[1], s=100, marker='+', linewidths=5)  \n",
    "\n",
    "\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals =-self.w[0]/self.w[1] * x_vals\n",
    "    plt.plot(x_vals, y_vals, '--')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Predict function\n",
    "Once we have found the optimum weight, we multiply them with test item features and predict as 1 or -1 based on the sign of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    y_pred = np.sum(self.w*x, axis=1)\n",
    "    return np.array([1 if y>0 else -1 for y in y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, l_rate=.00001, epoch=100000):\n",
    "        self.l_rate = l_rate\n",
    "        self.epoch = epoch\n",
    "    \n",
    "    def fit(self, x, y, plot=True):\n",
    "        self.w = np.zeros(len(x[0]))\n",
    "        \n",
    "        for e in range(self.epoch):\n",
    "    \n",
    "            for i, val in enumerate(x):\n",
    "                val1 = np.dot(x[i], self.w)\n",
    "                if (y[i]*val1 < 1):\n",
    "                    self.w = self.w + self.l_rate * ((y[i]*x[i]) - (2*(1/(e+1))*self.w))\n",
    "                else:\n",
    "                    self.w = self.w + self.l_rate * (-2*(1/(e+1))*self.w)\n",
    "        \n",
    "        if plot:\n",
    "            self.plot(x,y)\n",
    "            \n",
    "    def plot(self, x, y):\n",
    "        for val, inp in enumerate(x):\n",
    "            if y[val] == -1:\n",
    "                plt.scatter(inp[0], inp[1], s=100, marker='_', linewidths=5)\n",
    "            else:\n",
    "                plt.scatter(inp[0], inp[1], s=100, marker='+', linewidths=5)  \n",
    "\n",
    "\n",
    "        axes = plt.gca()\n",
    "        x_vals = np.array(axes.get_xlim())\n",
    "        y_vals =-self.w[0]/self.w[1] * x_vals\n",
    "        plt.plot(x_vals, y_vals, '--')   \n",
    "    \n",
    "    def predict(self, x):\n",
    "        y_pred = np.sum(self.w*x, axis=1)\n",
    "        return np.array([1 if y>0 else -1 for y in y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that above method is not the best way of implementing SVM for production and we have implemented a very simplified version to get an intuitive understanding of what goes behind the scene**. Normally, we use optimization libraries such as **cvxopt** to solve the optimization problem which does it in much more efficient and effective way. Please check out this [implementation](https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/supervised_learning/support_vector_machine.py) which is much more optimized and uses **cvxopt**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use only **2 features and 2 classes** from the iris data for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[:100,:2]\n",
    "y = y[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1 if i>0 else -1 for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = SVM()\n",
    "clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights of decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.33789749, -4.03062021])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",np.sum(y==clf.predict(x))*1.0/len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=1, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "skclf = SVC(kernel='linear', degree=1)\n",
    "skclf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.22720466, -2.24959915]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skclf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that our weights are different compared to sklearn's weight for decision boundary showing that out implementation is not optimized**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",np.sum(y==skclf.predict(x))*1.0/len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Trick\n",
    "In SVM, it is easy to have a linear hyper-plane between these two classes. But, another burning question which arises is, should we need to add this feature manually to have a hyper-plane. No, SVM has a technique called the kernel trick. These are functions which takes low dimensional input space and transform it to a higher dimensional space i.e. it converts not separable problem to separable problem, these functions are called kernels. It is mostly useful in non-linear separation problem. Simply put, it does some extremely complex data transformations, then find out the process to separate the data based on the labels or outputs you’ve defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons associated with SVM\n",
    "**Pros:**\n",
    "1. It works really well with clear margin of separation\n",
    "2. It is effective in high dimensional spaces.\n",
    "3. It is effective in cases where number of dimensions is greater than the number of samples.\n",
    "4. It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "\n",
    "**Cons:**\n",
    "1. It doesn’t perform well, when we have large data set because the required training time is higher\n",
    "2. It also doesn’t perform very well, when the data set has more noise i.e. target classes are overlapping\n",
    "3. SVM doesn’t directly provide probability estimates, these are calculated using an expensive five-fold cross-validation. It is related SVC method of Python scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check out this [blog](https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72) for better understanding and various parameters to tune for SVM.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
