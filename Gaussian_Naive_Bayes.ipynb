{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an extension to [Naive Bayes Notebook](https://github.com/jyotipmahes/Implementation-of-ML-algos-in-Python/blob/master/Naive_Bayes.ipynb). In last notebook, we implemented Naive Bayes from scratch for categorical variables. In this notebook, we will discuss how to **extend Naive Bayes to continuous numerical data**. In Gaussian Naive Bayes, we make an **assumption** that the **numerical features follow a normal distribution** and we can calculate the **conditional probabilities P(x/c) with the help of Normal density function**. Here are the steps we will follow to implement Gaussian Naive Bayes.\n",
    "1. **Segregate** the data as per **classes** and calculate the **mean** and **standard deviation** for each numerical feature. Also **calculate class prior** probabilities similar to previous notebook.\n",
    "2. For any test data, calculate the **conditional probability** for each feature values with the help of **Normal density function** and using the mean and standard deviation calculate per class. \n",
    "3. **Multiply** the **conditional probabilities** and multiply it with **prior probabilities** (and take log) to get posterior probabilities. \n",
    "4. **Select** the **class** with **highest probabilities**.\n",
    "\n",
    "As we can see, the only difference is the way we calculate conditional probabilities and rest of the process remains the same. Hence we will use the previous code with slight modifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://images.slideplayer.com/25/7828801/slides/slide_14.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "from functools import reduce\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNB:\n",
    "    def __init__(self):\n",
    "        self.n_items = None\n",
    "        self.classes = None\n",
    "        self.priors = defaultdict(float)\n",
    "        self.prob_predicted = defaultdict(float)\n",
    "        self.class_feature_mean = defaultdict(list)\n",
    "        self.class_feature_std = defaultdict(list)\n",
    "    \n",
    "    def fit(self, x_train, labels):\n",
    "        self.x_train = np.array(x_train)\n",
    "        self.labels = np.array(labels)\n",
    "        self.n_items = self.labels.size\n",
    "        self.classes = set(self.labels)\n",
    "        \n",
    "        for i in self.classes:\n",
    "            self.class_feature_mean[i] = np.mean(x_train[labels==i], axis = 0)\n",
    "            self.class_feature_std[i] = np.std(x_train[labels==i], axis = 0)\n",
    "            \n",
    "        self.doc_priors()\n",
    "\n",
    "    \n",
    "    def doc_priors(self):\n",
    "        for label in self.classes:\n",
    "            self.priors[label] = np.sum(1 for d in self.labels if d == label)*1.0 / self.n_items\n",
    "    \n",
    "    def likelihood(self, label, x):\n",
    "        exponent = np.exp(-((x- self.class_feature_mean[label])**2/\n",
    "                            (2*(self.class_feature_std[label])**2)))\n",
    "        probs =  (1 / (np.sqrt(2*np.pi) * self.class_feature_std[label])) * exponent\n",
    "        return np.sum(np.log(probs))\n",
    "    \n",
    "    \n",
    "    def post_prob(self, test):\n",
    "        for label in self.classes:\n",
    "            self.prob_predicted[label] = np.log(self.priors[label]) \n",
    "            self.prob_predicted[label] += self.likelihood(label, test)\n",
    "        return self.prob_predicted\n",
    "\n",
    "   \n",
    "    def predict(self, test):\n",
    "        self.test_labels = []\n",
    "        for i in test:\n",
    "            prob_predicted = self.post_prob(i)\n",
    "            label, prob = max(prob_predicted.items(),\n",
    "                      key=itemgetter(1))\n",
    "            self.test_labels.append(label)\n",
    "        return self.test_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking our implementation on iris data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get train and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size = 0.3, \n",
    "                                                    random_state =1, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating classifier and fitting to train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy 0.933\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X_test)\n",
    "print(\"Prediction accuracy %.3f\" %(np.sum(pred==y_test)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like out implementation is correct and we have build a good enough classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compairing with sklearn predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB as sGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = sGB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy 0.933\n"
     ]
    }
   ],
   "source": [
    "pred = clf2.predict(X_test)\n",
    "print(\"Prediction accuracy %.3f\" %(np.sum(pred==y_test)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get same results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BernoulliNB implements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable. Therefore, this class requires samples to be represented as binary-valued feature vectors. The difference between Bernoulli naive Bayes and Multinomial Naive Bayes is the way we calculate likelihood function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(x_i|y)= P(i|y)x_i + (1- P(i|y))(1- x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which differs from multinomial NBâ€™s rule in that it explicitly penalizes the non-occurrence of a feature  that is an indicator for class , where the multinomial variant would simply ignore a non-occurring feature.\n",
    "\n",
    "In the case of text classification, word occurrence vectors (rather than word count vectors) may be used to train and use this classifier. BernoulliNB might perform better on some datasets, especially those with shorter documents.\n",
    "\n",
    "**We can implement this by changing the likelihood function in Multinomial Naive Bayes implementation shared in last notebook**.\n",
    "\n",
    "Please refer to this link to check the [implementation](https://mattshomepage.com/articles/2016/Jun/07/bernoulli_nb/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixing Gaussian and Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen the implementation of both Gaussian and Multinomial Naive Bayes, we can mix them together and create a hybrid classifier which can handle both **Categorical** and **continuous** data. We have to define t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
