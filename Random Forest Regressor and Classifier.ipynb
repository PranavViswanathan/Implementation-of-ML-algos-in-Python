{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision trees** involve the **greedy selection** of the **best split** point from the dataset at each step.\n",
    "\n",
    "This algorithm makes decision trees susceptible to **high variance** if they are not pruned. This high variance can be harnessed and reduced by creating **multiple trees with different samples** of the training dataset (different views of the problem) and combining their predictions. This approach is called **bootstrap aggregation or bagging for short**.\n",
    "\n",
    "A **limitation of bagging** is that the **same greedy algorithm** is used to create each tree, meaning that it is likely that the same or **very similar split** points will be chosen in each tree making the **different trees very similar** (trees will be correlated). This, in turn, **makes their predictions similar, mitigating the variance originally sought.**\n",
    "\n",
    "We can **force the decision trees to be different by limiting the features (rows)** that the greedy algorithm can evaluate at each split point **when creating the tree**. This is called the **Random Forest algorithm**.\n",
    "\n",
    "**Like bagging**, **multiple samples** of the training dataset are taken and a **different tree trained** on each. The difference is that at each point a split is made in the data and added to the tree, only a **fixed subset of attributes can be considered.**\n",
    "\n",
    "For classification problems, the type of problems we will look at in this tutorial, the number of attributes to be considered for the split is limited to the square root of the number of input features.\n",
    "\n",
    "The result of this one small change are **trees that are more different** from each other (uncorrelated) resulting predictions that are more diverse and a combined prediction that often **has better performance that single tree** or bagging alone.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are creating a random forest regressor, although the same code can be slightly modified to create a classifier. \n",
    "\n",
    "The parameters that define our random forest are:\n",
    "\n",
    "**x:** independent variables of training set.<br>\n",
    "**y:** the corresponding dependent variables necessary for supervised learning  <br>\n",
    "**n_trees :** number of uncorrelated trees we ensemble to create the random forest. <br>\n",
    "**n_features:** the number of features to sample and pass onto each tree, this is where feature bagging happens. It can either be sqrt, log2 or an integer. In case of sqrt, the number of features sampled to each tree is square root of total features and log base 2 of total features in case of log2.<br>\n",
    "**sample_size:** the number of rows randomly selected and passed onto each tree. This is usually equal to total number of rows but can be reduced to increase performance and decrease correlation of trees in some cases<br>\n",
    "**depth:** depth of each decision tree. Higher depth means more number of splits which increases the over fitting tendency of each tree but since we are aggregating several uncorrelated trees, over fitting of individual trees hardly bothers the whole forest.<br>\n",
    "**min_leaf:** minimum number of rows required in a leaf node. Reduces the chances of over fitting<br>\n",
    "**seed**: Seed for pseudo random number generator. <br>\n",
    "\n",
    "We have already discussed Decision Tree Regressor in this [notebook](https://github.com/jyotipmahes/Implementation-of-ML-algos-in-Python/blob/master/Decision%20Tree%20Regression%20.ipynb). Random Forest is a slight extension of this. Please refer to it for detailed explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initializing Random Forest Regressor\n",
    "\n",
    "We will start by creating as class for RandomForest and initialize it. Note we will not create separate fit function for the sake of brevity and do it during initialization. We start by initializing `x`, `y`, `n_trees`, `n_features`, `sample_sz`, `depth` and `min_leaf` and `seed` as discussed above. We also call a function called `self.create_tree()` `n_trees` number of times to create `n_trees`. We will look at the `self.create_tree()` function definition next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, x, y, n_trees, n_features, sample_sz, depth=10, min_leaf=5, seed=10):\n",
    "        np.random.seed(seed)\n",
    "        if n_features == 'sqrt':\n",
    "            self.n_features = int(np.sqrt(x.shape[1]))\n",
    "        elif n_features == 'log2':\n",
    "            self.n_features = int(np.log2(x.shape[1]))\n",
    "        else:\n",
    "            self.n_features = n_features\n",
    "        print(self.n_features, \"sha: \",x.shape[1])    \n",
    "        self.x, self.y, self.sample_sz, self.depth, self.min_leaf  = x, y, sample_sz, depth, min_leaf\n",
    "        self.trees = [self.create_tree() for i in range(n_trees)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating trees\n",
    "`create_tree` function helps us create a set of decision trees. Here are things we do in this function:\n",
    "1. We shuffle the rows and select rows equal to `sample_sz`.\n",
    "2. We randomly select feature columns equal to `n_features`.\n",
    "3. Now we call DecisionTree constructor to create decision tree with selected feature columns, selected rows, given `depth` and `min_leaf`.\n",
    "\n",
    "Implementation of DecisionTree is discussed in details in previous [notebook](https://github.com/jyotipmahes/Implementation-of-ML-algos-in-Python/blob/master/Decision%20Tree%20Regression%20.ipynb). We will quickly revisit the implementation next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tree(self):\n",
    "    idxs = np.random.permutation(len(self.y))[:self.sample_sz]\n",
    "    f_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "    return DecisionTree(self.x.iloc[idxs], self.y[idxs], self.n_features, f_idxs,\n",
    "                idxs=np.array(range(self.sample_sz)),depth = self.depth, min_leaf=self.min_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementation of Decision Tree Regressor\n",
    "Implementation of Decision Tree Regressor is already discussed in detail in [notebook](https://github.com/jyotipmahes/Implementation-of-ML-algos-in Python/blob/master/Decision%20Tree%20Regression%20.ipynb). Here is the brief steps:\n",
    "1. Implement function to calculate **mean squared error** using `std_agg` which is the splitting criterion for regression.\n",
    "2. Initialize a Decision Tree with given inputs.\n",
    "3. Look for best split across columns/features using `find_varsplit()` and `find_better_split()`. Create new binary split based on the results of finding best split.\n",
    "4. Defining properties such as `is_leaf`, `split_name` and `split_col` for ease of implementation.\n",
    "5. Implementing `predict_row` function which transverse the tree add find the mean of the leaf node corresponding to test data features. `predict` is a wrapper to predict for all test data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_agg(cnt, s1, s2): \n",
    "    return (s2/cnt) - (s1/cnt)**2\n",
    "\n",
    "class DecisionTree():\n",
    "    def __init__(self, x, y, n_features, f_idxs,idxs,depth=5, min_leaf=5):\n",
    "        self.x, self.y, self.idxs, self.min_leaf, self.f_idxs = x, y, idxs, min_leaf, f_idxs\n",
    "        self.depth = depth\n",
    "        self.n_features = n_features\n",
    "        self.n, self.c = len(idxs), x.shape[1]\n",
    "        self.val = np.mean(y[idxs])\n",
    "        self.score = float('inf')\n",
    "        self.find_varsplit()\n",
    "        \n",
    "    def find_varsplit(self):\n",
    "        for i in self.f_idxs: self.find_better_split(i)\n",
    "        if self.is_leaf: return\n",
    "        x = self.split_col\n",
    "        lhs = np.nonzero(x<=self.split)[0]\n",
    "        rhs = np.nonzero(x>self.split)[0]\n",
    "        lf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        rf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        self.lhs = DecisionTree(self.x, self.y, self.n_features, lf_idxs, \n",
    "                                self.idxs[lhs], depth=self.depth-1, min_leaf=self.min_leaf)\n",
    "        self.rhs = DecisionTree(self.x, self.y, self.n_features, rf_idxs, \n",
    "                                self.idxs[rhs], depth=self.depth-1, min_leaf=self.min_leaf)\n",
    "\n",
    "    def find_better_split(self, var_idx):\n",
    "        x, y = self.x.values[self.idxs,var_idx], self.y[self.idxs]\n",
    "        sort_idx = np.argsort(x)\n",
    "        sort_y,sort_x = y[sort_idx], x[sort_idx]\n",
    "        rhs_cnt,rhs_sum,rhs_sum2 = self.n, sort_y.sum(), (sort_y**2).sum()\n",
    "        lhs_cnt,lhs_sum,lhs_sum2 = 0,0.,0.\n",
    "\n",
    "        for i in range(0,self.n-self.min_leaf-1):\n",
    "            xi,yi = sort_x[i],sort_y[i]\n",
    "            lhs_cnt += 1; rhs_cnt -= 1\n",
    "            lhs_sum += yi; rhs_sum -= yi\n",
    "            lhs_sum2 += yi**2; rhs_sum2 -= yi**2\n",
    "            if i<self.min_leaf or xi==sort_x[i+1]:\n",
    "                continue\n",
    "\n",
    "            lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n",
    "            rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n",
    "            curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt\n",
    "            if curr_score<self.score: \n",
    "                self.var_idx,self.score,self.split = var_idx,curr_score,xi\n",
    "\n",
    "    @property\n",
    "    def split_name(self): return self.x.columns[self.var_idx]\n",
    "    \n",
    "    @property\n",
    "    def split_col(self): return self.x.values[self.idxs,self.var_idx]\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self): return self.score == float('inf') or self.depth <= 0 \n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.array([self.predict_row(xi) for xi in x])\n",
    "\n",
    "    def predict_row(self, xi):\n",
    "        if self.is_leaf: return self.val\n",
    "        t = self.lhs if xi[self.var_idx]<=self.split else self.rhs\n",
    "        return t.predict_row(xi)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implementing predict function for random forest\n",
    "Since we have the predict function implementation at Decision Tree level, implementing predict function for random forest is very easy. We just **call the predict function on all Decision Trees and take a mean** of their values. We can use other functions such as **median, mode or some custom functions** if we wish show. Here is the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    return np.mean([t.predict(x) for t in self.trees], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class RandomForest():\n",
    "    def __init__(self, x, y, n_trees, n_features, sample_sz, depth=10, min_leaf=5):\n",
    "        np.random.seed(12)\n",
    "        if n_features == 'sqrt':\n",
    "            self.n_features = int(np.sqrt(x.shape[1]))\n",
    "        elif n_features == 'log2':\n",
    "            self.n_features = int(np.log2(x.shape[1]))\n",
    "        else:\n",
    "            self.n_features = n_features\n",
    "        print(self.n_features, \"sha: \",x.shape[1])    \n",
    "        self.x, self.y, self.sample_sz, self.depth, self.min_leaf  = x, y, sample_sz, depth, min_leaf\n",
    "        self.trees = [self.create_tree() for i in range(n_trees)]\n",
    "\n",
    "    def create_tree(self):\n",
    "        idxs = np.random.permutation(len(self.y))[:self.sample_sz]\n",
    "        f_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        return DecisionTree(self.x.iloc[idxs], self.y[idxs], self.n_features, f_idxs,\n",
    "                    idxs=np.array(range(self.sample_sz)),depth = self.depth, min_leaf=self.min_leaf)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.mean([t.predict(np.array(x)) for t in self.trees], axis=0)\n",
    "\n",
    "def std_agg(cnt, s1, s2): \n",
    "    return (s2/cnt) - (s1/cnt)**2\n",
    "\n",
    "class DecisionTree():\n",
    "    def __init__(self, x, y, n_features, f_idxs,idxs,depth=10, min_leaf=5):\n",
    "        self.x, self.y, self.idxs, self.min_leaf, self.f_idxs = x, y, idxs, min_leaf, f_idxs\n",
    "        self.depth = depth\n",
    "        self.n_features = n_features\n",
    "        self.n, self.c = len(idxs), x.shape[1]\n",
    "        self.val = np.mean(y[idxs])\n",
    "        self.score = float('inf')\n",
    "        self.find_varsplit()\n",
    "        \n",
    "    def find_varsplit(self):\n",
    "        for i in self.f_idxs: self.find_better_split(i)\n",
    "        if self.is_leaf: return\n",
    "        x = self.split_col\n",
    "        lhs = np.nonzero(x<=self.split)[0]\n",
    "        rhs = np.nonzero(x>self.split)[0]\n",
    "        lf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        rf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        self.lhs = DecisionTree(self.x, self.y, self.n_features, lf_idxs, self.idxs[lhs], depth=self.depth-1, min_leaf=self.min_leaf)\n",
    "        self.rhs = DecisionTree(self.x, self.y, self.n_features, rf_idxs, self.idxs[rhs], depth=self.depth-1, min_leaf=self.min_leaf)\n",
    "\n",
    "    def find_better_split(self, var_idx):\n",
    "        x, y = self.x.values[self.idxs,var_idx], self.y[self.idxs]\n",
    "        sort_idx = np.argsort(x)\n",
    "        sort_y,sort_x = y[sort_idx], x[sort_idx]\n",
    "        rhs_cnt,rhs_sum,rhs_sum2 = self.n, sort_y.sum(), (sort_y**2).sum()\n",
    "        lhs_cnt,lhs_sum,lhs_sum2 = 0,0.,0.\n",
    "\n",
    "        for i in range(0,self.n-self.min_leaf-1):\n",
    "            xi,yi = sort_x[i],sort_y[i]\n",
    "            lhs_cnt += 1; rhs_cnt -= 1\n",
    "            lhs_sum += yi; rhs_sum -= yi\n",
    "            lhs_sum2 += yi**2; rhs_sum2 -= yi**2\n",
    "            if i<self.min_leaf or xi==sort_x[i+1]:\n",
    "                continue\n",
    "\n",
    "            lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n",
    "            rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n",
    "            curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt\n",
    "            if curr_score<self.score: \n",
    "                self.var_idx,self.score,self.split = var_idx,curr_score,xi\n",
    "\n",
    "    @property\n",
    "    def split_name(self): return self.x.columns[self.var_idx]\n",
    "    \n",
    "    @property\n",
    "    def split_col(self): return self.x.values[self.idxs,self.var_idx]\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self): return self.score == float('inf') or self.depth <= 0 \n",
    "    \n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.array([self.predict_row(xi) for xi in x])\n",
    "\n",
    "    def predict_row(self, xi):\n",
    "        if self.is_leaf: return self.val\n",
    "        t = self.lhs if xi[self.var_idx]<=self.split else self.rhs\n",
    "        return t.predict_row(xi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note this code is based on Fastai [Machine learning course](https://www.fast.ai/2018/09/26/ml-launch/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "bos = load_boston()\n",
    "x = bos.data\n",
    "y = bos.target\n",
    "x_df = pd.DataFrame(x, columns=bos.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 sha:  12\n"
     ]
    }
   ],
   "source": [
    "reg = RandomForest(x_df.iloc[:,:-1], y, n_trees=10, n_features='sqrt', sample_sz=len(y), depth=10, min_leaf=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = reg.predict(x_df.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.919806291006816"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((pred - y)**2)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing to sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "skreg = RandomForestRegressor(min_samples_leaf=5, max_depth=10, max_features='sqrt', n_estimators=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=10,\n",
       "           max_features='sqrt', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=5, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skreg.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.999090363190955"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = skreg.predict(x)\n",
    "sum((pred - y)**2)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that our implementation is very similar to sklearn implementation in terms of prediction.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons\n",
    "**Pros**\n",
    "1. This algorithm can solve both type of problems i.e. classification and regression and does a decent estimation at both fronts.\n",
    "2. One of benefits of Random forest which excites me most is, the power of handle large data set with higher dimensionality. Further, the model outputs Importance of variable, which can be a very handy feature (on some random data set).\n",
    "3. It has methods for balancing errors in data sets where classes are imbalanced. The capabilities of the above can be extended to unlabeled data, leading to unsupervised clustering, data views and outlier detection.\n",
    "4. **Out of Bag Error**:Random Forest involves sampling of the input data with replacement called as bootstrap sampling. Here one third of the data is not used for training and can be used to testing. These are called the out of bag samples. Error estimated on these out of bag samples is known as out of bag error. Study of error estimates by Out of bag, gives evidence to show that the out-of-bag estimate is as accurate as using a test set of the same size as the training set. Therefore, using the out-of-bag error estimate removes the need for a set aside test set.\n",
    " \n",
    "\n",
    "**Cons**\n",
    "1. It surely does a good job at classification but not as good as for regression problem as it does not give precise continuous nature predictions. In case of regression, it doesn’t predict beyond the range in the training data, and that they may over-fit data sets that are particularly noisy.\n",
    "2. Random Forest can feel like a black box approach for statistical modelers. You have very little control on what the model does. You can at best try different parameters and random seeds!\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "Implementation of Random Forest Classifier is very similar to Regressor we saw above. We just need to change the Decision Tree classifier similar to this [notebook](https://github.com/jyotipmahes/Implementation-of-ML-algos-in-Python/blob/master/Decision%20Tree%20Classifier.ipynb) and slightly modify the Random Forest implementation to accommodate and act as a classifier. Here is the slight changes we make:\n",
    "1. Change the split criterion as Gini Index\n",
    "2. Prediction is done by maximum vote.\n",
    "3. We are not implementing min_leaf condition here as we are using our previous implementation of decision tree classifier. \n",
    "\n",
    "Here is the code for **Randomforest classifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "class RandomForestClassifier():\n",
    "    def __init__(self, x, target, n_trees, n_features, sample_sz=None, depth=10):\n",
    "        if sample_sz == None: sample_sz = x.shape[0]\n",
    "        np.random.seed(12)\n",
    "        if n_features == 'sqrt':\n",
    "            self.n_features = int(np.sqrt(x.shape[1]))\n",
    "        elif n_features == 'log2':\n",
    "            self.n_features = int(np.log2(x.shape[1]))\n",
    "        else:\n",
    "            self.n_features = n_features\n",
    "        print(self.n_features, \"sha: \",x.shape[1])    \n",
    "        self.x, self.target, self.sample_sz, self.depth  = x, target, sample_sz, depth\n",
    "        self.trees = [self.create_tree() for i in range(n_trees)]\n",
    "\n",
    "    def create_tree(self):\n",
    "        idxs = np.random.permutation(self.x.shape[0])[:self.sample_sz]\n",
    "        f_idxs = np.random.permutation(self.x.shape[1]-1)[:self.n_features]\n",
    "        f_idxs = list(f_idxs)\n",
    "        f_idxs.append(self.x.shape[1]-1)\n",
    "        f_idxs = np.array(f_idxs)\n",
    "        clf = DecisionTree(self.depth)\n",
    "        clf.fit(self.x.iloc[idxs,f_idxs], self.target)\n",
    "        return clf\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return mode([t.predict(x) for t in self.trees], axis=0)[0][0]\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, max_depth = 5, depth = 1):\n",
    "        self.max_depth = max_depth\n",
    "        self.depth = depth\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "    \n",
    "    def fit(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.columns = self.data.columns.tolist()\n",
    "        self.columns.remove(target)\n",
    "        if self.depth <= self.max_depth:\n",
    "            self.__validate_data()\n",
    "            self.impurity_score = self.__calculate_impurity_score(self.data[self.target])\n",
    "            self.criteria, self.split_feature, self.information_gain = self.__find_best_split()\n",
    "            if self.criteria is not None and self.information_gain > 0: self.__create_branches()\n",
    "\n",
    "    \n",
    "    def __create_branches(self):\n",
    "        self.left = DecisionTree(max_depth = self.max_depth, \n",
    "                                 depth = self.depth + 1)\n",
    "        self.right = DecisionTree(max_depth = self.max_depth, \n",
    "                                 depth = self.depth + 1)\n",
    "        left_rows = self.data[self.data[self.split_feature] <= self.criteria] \n",
    "        right_rows = self.data[self.data[self.split_feature] > self.criteria] \n",
    "        self.left.fit(data = left_rows, target = self.target)\n",
    "        self.right.fit(data = right_rows, target = self.target)\n",
    "    \n",
    "    def __calculate_impurity_score(self, data):\n",
    "        if data is None or data.empty: return 0\n",
    "        gini_impurity = 1.\n",
    "        classes = np.unique(data)\n",
    "        n_vals = len(data)\n",
    "        for cl in classes:\n",
    "            gini_impurity -= np.square(np.sum(data==cl)/n_vals)  \n",
    "        return gini_impurity\n",
    "    \n",
    "    def __find_best_split(self):\n",
    "        best_split = {}\n",
    "        for col in self.columns:\n",
    "            information_gain, split = self.__find_best_split_for_column(col)\n",
    "            if split is None: continue\n",
    "            if not best_split or best_split[\"information_gain\"] < information_gain:\n",
    "                best_split = {\"split\": split, \"col\": col, \"information_gain\": information_gain}\n",
    "\n",
    "        return best_split.get(\"split\"), best_split.get(\"col\"), best_split.get(\"information_gain\")\n",
    "\n",
    "    def __find_best_split_for_column(self, col):\n",
    "        x = self.data[col]\n",
    "        unique_values = x.unique()\n",
    "        if len(unique_values) == 1: return None, None\n",
    "        information_gain = None\n",
    "        split = None\n",
    "        for val in unique_values:\n",
    "            left = x <= val\n",
    "            right = x > val\n",
    "            left_data = self.data[left]\n",
    "            right_data = self.data[right]\n",
    "            left_impurity = self.__calculate_impurity_score(left_data[self.target])\n",
    "            right_impurity = self.__calculate_impurity_score(right_data[self.target])\n",
    "            score = self.__calculate_information_gain(left_count = len(left_data),\n",
    "                                                      left_impurity = left_impurity,\n",
    "                                                      right_count = len(right_data),\n",
    "                                                      right_impurity = right_impurity)\n",
    "            if information_gain is None or score > information_gain: \n",
    "                information_gain = score \n",
    "                split = val\n",
    "        return information_gain, split\n",
    "    \n",
    "    def __calculate_information_gain(self, left_count, left_impurity, right_count, right_impurity):\n",
    "        return self.impurity_score - ((left_count/len(self.data)) * left_impurity + \\\n",
    "                                      (right_count/len(self.data)) * right_impurity)\n",
    "\n",
    "    def predict(self, data):\n",
    "        return np.array([self.__flow_data_thru_tree(row) for _, row in data.iterrows()])\n",
    "\n",
    "    def __validate_data(self):\n",
    "        non_numeric_columns = self.data[self.columns].select_dtypes(include=\n",
    "                                ['category', 'object', 'bool']).columns.tolist()\n",
    "        if(len(set(self.columns).intersection(set(non_numeric_columns))) != 0):\n",
    "            raise RuntimeError(\"Not all columns are numeric\")\n",
    "\n",
    "    def __flow_data_thru_tree(self, row):\n",
    "        if self.is_leaf_node: return self.probability\n",
    "        tree = self.left if row[self.split_feature] <= self.criteria else self.right\n",
    "        return tree.__flow_data_thru_tree(row)\n",
    "        \n",
    "    @property\n",
    "    def is_leaf_node(self): return self.left is None\n",
    "\n",
    "    @property\n",
    "    def probability(self): \n",
    "        return mode(self.data[self.target])[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking our implementation on iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 sha:  5\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(iris_df, 'target', 10, 'sqrt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 1.000\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(iris_df)\n",
    "print(\"Accuracy %.3f\" %(np.sum(pred==iris.target)*1.0/len(iris.target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that our implementation of the classifier is correct.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
